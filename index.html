<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="ActFormer">
    <meta name="keywords" content="Detection, Collaborative Perception, Robotic exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <title>AirDet</title> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
    <link rel="icon" type="image/png" href="./static/images/ai4ce.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/ai4ce.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jaraxxus-me.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://ieeexplore.ieee.org/document/9561564">
            ADTrack - ICRA 2024
          </a>
          <a class="navbar-item" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Cao_HiFT_Hierarchical_Feature_Transformer_for_Aerial_Tracking_ICCV_2021_paper.pdf">
            HiFT - ICRA 2024
          </a>
        </div>
      </div>
    </div> -->

        </div>
    </nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <!-- <h1 class="title is-1 publication-title"><img src="./static/images/drone.svg" width="120">AirDet&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</h1> -->
                        <h1 class="title is-2 publication-title">ActFormer: Scalable Collaborative Perception via Active
                            Queries</h1>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://hsz0403.github.io">Suozhi Huang*</a><sup>2</sup>,</span>

                            <span class="author-block">
                                <a href="https://juexzz.github.io">Juexiao Zhang*</a><sup>1</sup>,</span>

                            <span class="author-block">
                                <a href="https://roboticsyimingli.github.io">Yiming Li*</a><sup>1</sup>,</span>

                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Chen
                                    Feng</a><sup>1</sup>
                            </span>

                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>New York University</span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>2</sup>Tsinghua University</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->

                                <!-- <span class="link-block"> -->
                                <!-- <a href="https://openreview.net/forum?id=hW0tcXOJas2" -->
                                <!-- class="external-link button is-normal is-rounded is-dark"> -->
                                <!-- <span class="icon"> -->
                                <!-- <i class="ai ai-arxiv"></i> -->
                                <!-- </span> -->
                                <!-- <span>OpenReview</span> -->
                                <!-- </a> -->
                                <!-- </span> -->
                                <!-- Video Link. -->
                                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/coperception/ActFormer"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>Video(Coming)</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <!-- <span class="link-block">
                <a href="https://github.com/Jaraxxus-Me/AirDet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> -->
                                <!-- <span class="link-block">
                <a href="https://github.com/Jaraxxus-Me/AirDet_ROS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>ROS</span>
                </a>
              </span> -->
                                <!-- <span class="link-block">
                <a href="https://zhuanlan.zhihu.com/p/545249730"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-blog"></i>
                  </span>
                  <span>Blog</span>
                </a>
              </span> -->
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Paper video. -->
            <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/V5hYTz5os0M?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
            <!--/ Paper video. -->

            <!-- <br> -->
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Collaborative perception leverages rich visual observations from multiple robots to extend a
                            single robot's perception ability beyond its field of view.
                            Many prior works receive messages broadcast from all collaborators, leading to a scalability
                            challenge when dealing with a large number of robots and sensors.

                            In this work, we aim to address scalable camera-based collaborative perception with a
                            Transformer-based architecture. Our key idea is to enable a single robot to intelligently
                            discern the relevance of the collaborators and their associated cameras according to a
                            learned spatial prior. This proactive understanding of the visual features' relevance does
                            not require the transmission of the features themselves, enhancing both communication and
                            computation efficiency. Specifically, we present ActFormer, a Transformer that learns bird's
                            eye view (BEV) representations by using predefined BEV queries to interact with multi-robot
                            multi-camera inputs. Each BEV query can actively select relevant cameras for information
                            aggregation based on pose information, instead of interacting with all cameras
                            indiscriminately. Experiments on the V2X-Sim dataset demonstrate that ActFormer improves the
                            detection performance from 29.89\% to 45.15\% in terms of AP@0.7 with about 50\% fewer
                            queries, showcasing the effectiveness of ActFormer in multi-agent collaborative 3D object
                            detection.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- <br> -->
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Contribution</h2>
                    <div class="content has-text-justified">
                        <li>
                            We conceptualize a scalable and efficient collaborative perception framework that can
                            actively and intelligently identify the most relevant sensory measurements based on spatial
                            knowledge, without relying on the sensory measurements themselves.
                        </li>
                        <li>
                            We ground the concept of the scalable collaborative perception with a Transformer,
                            \textit{i.e.}, \textit{ActFormer}, which uses a group of 3D-to-2D BEV queries to actively
                            and efficiently aggregate the features from multi-robot multi-camera input, only relying on
                            pose information.
                        </li>
                        <li>
                            We conduct comprehensive experiments in the task of collaborative object detection to verify
                            the effectiveness and efficiency of our ActFormer.</b>.

                        </li>

                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Method. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-full_width">
                    <hr>
                    <h2 class="title is-3">Method</h2>
                    <br>
                    <img src="static\images\actformer.png" class="center" />
                    <div class="content has-text-justified">
                        <br>
                        <p>


                            Our motivation stems from the idea that how vehicles collaboratively perceive should be
                            closely related to their relative poses. Different camera poses result in varying
                            viewpoints, each capturing unique information. However, conventional collaborative methods
                            often treat all viewpoints equally, overlooking the fact that these camera perspectives
                            offer different insights into the environment—some unique, some overlapping, and some
                            redundant. Consequently, the ego vehicle may not fully capitalize on the diverse
                            perspectives available, leading to indiscriminate collaboration that generates excessive
                            communication and computation. Actually, communication may not be necessary when some
                            partners share very similar observations.
                        </p>
                    </div>
                </div>
            </div>
            <hr>








            <!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    The work was done when Bowen Li and Pranay Reddy were interns at The Robotics Institute, CMU. The authors would like to thank all members of the Team Explorer for providing data collected from the DARPA Subterranean Challenge. Our code is built upon <a href="https://github.com/fanq15/FewX">FewX</a>, for which we sincerely express our gratitute to the authors.
  </div>
</section> -->


            <footer class="footer">
                <div class="container">
                    <div class="content has-text-centered">
                    </div>
                    <div class="columns is-centered">
                        <div class="column is-8">
                            <div class="content">
                                <p>
                                    This website is licensed under a <a rel="license"
                                        href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                        Commons Attribution-ShareAlike 4.0 International License</a>.
                                    This webpage template is from <a
                                        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                                    We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing
                                    and
                                    open-sourcing this template.
                                </p>
                            </div>
                        </div>
                        </p>
                    </div>
                </div>
        </div>
        </div>
        </footer>

</body>

</html>